---
title: "analysis"
output: html_document
date: "2025-06-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(arrow)
library(ggplot2)
library(tidyr)
library(dplyr)
library(tidymodels)
library(data.table)
library(bonsai)
```

This is an WIP analysis of the effects of fundamentals on returns in excess of the markets. 
Most data processing is done through run_pipeline.py in src. Data processing is not final and is contiunually undergoing more processing. 

This file provides a shell to run a lightgbm model with fundamentals as variables on the returns of equities in excess of market returns (where market returns is the returns on VTI.) The effects of each variable will be studied through SHAP values. 

## Reading in Data

```{r}
df = read_parquet("processed_data/monthly_data.parquet")
```

# Generating summary of outliers
```{r}
columns_to_analyze <- c('pe', 'excess_close_rolling_return_1p', 'market_close_rolling_return_1p', 'evebitda', 'grossmargin', 'ebitdamargin', 'netmargin', 'currentratio')

# Initialize an empty list to store the summary for each column
results_list <- list()
total_rows_in_df <- nrow(df)

# Loop through each specified column name
for (col_name in columns_to_analyze) {
  current_col_data <- df[[col_name]]

  valid_data <- current_col_data[!is.na(current_col_data)]
  q_vals <- quantile(valid_data, probs = c(0.25, 0.75), na.rm = FALSE, type = 7)
  q1_val <- q_vals[1]
  q3_val <- q_vals[2]
  iqr_val <- q3_val - q1_val

  lower_bound <- q1_val - 3 * iqr_val
  upper_bound <- q3_val + 3 * iqr_val

  is_lower_outlier <- !is.na(current_col_data) & current_col_data < lower_bound
  is_upper_outlier <- !is.na(current_col_data) & current_col_data > upper_bound
  is_non_outlier   <- !is.na(current_col_data) & current_col_data >= lower_bound & current_col_data <= upper_bound
  
  lower_outlier_values <- current_col_data[is_lower_outlier]
  upper_outlier_values <- current_col_data[is_upper_outlier]

  num_non_outliers_val <- sum(is_non_outlier)
  
  num_lower_outliers_val <- sum(is_lower_outlier)
  percent_lower_outliers_val <- if (total_rows_in_df > 0) (num_lower_outliers_val / total_rows_in_df) else 0
  
  avg_lower_outlier_val    <- if (num_lower_outliers_val > 0) mean(lower_outlier_values) else NA_real_
  median_lower_outlier_val <- if (num_lower_outliers_val > 0) median(lower_outlier_values) else NA_real_
  max_lower_outlier_val    <- if (num_lower_outliers_val > 0) max(lower_outlier_values) else NA_real_ 

  num_upper_outliers_val <- sum(is_upper_outlier)
  percent_upper_outliers_val <- if (total_rows_in_df > 0) (num_upper_outliers_val / total_rows_in_df) else 0
  
  avg_upper_outlier_val    <- if (num_upper_outliers_val > 0) mean(upper_outlier_values) else NA_real_
  median_upper_outlier_val <- if (num_upper_outliers_val > 0) median(upper_outlier_values) else NA_real_
  min_upper_outlier_val    <- if (num_upper_outliers_val > 0) min(upper_outlier_values) else NA_real_

  col_summary <- data.frame(
    column_name = col_name,
    num_non_outliers = num_non_outliers_val,
    num_lower_outliers = num_lower_outliers_val,
    percent_lower_outliers = percent_lower_outliers_val,
    avg_lower_outlier_value = avg_lower_outlier_val,
    median_lower_outlier_value = median_lower_outlier_val,
    max_lower_outlier_value = max_lower_outlier_val,
    num_upper_outliers = num_upper_outliers_val,
    percent_upper_outliers = percent_upper_outliers_val,
    avg_upper_outlier_value = avg_upper_outlier_val,
    median_upper_outlier_value = median_upper_outlier_val,
    min_upper_outlier_value = min_upper_outlier_val,
    stringsAsFactors = FALSE
  )
  results_list[[col_name]] <- col_summary
}

outlier_summary_df <- dplyr::bind_rows(results_list)
print(outlier_summary_df)
```

# Defining LGB model.

```{r}
lgb_columns <- c('pe', 'excess_close_rolling_return_1p', 'evebitda', 'grossmargin', 'ebitdamargin', 'netmargin', 'currentratio')

# Quick final processing
lgb_dataset <- df %>%
  setDT() %>%
  select(lgb_columns) %>%
  mutate_all(function(x) ifelse(is.nan(x), NA, x)) %>%
  drop_na()

# Splitting data
lgb_split <- initial_split(lgb_dataset, strata = excess_close_rolling_return_1p)
lgb_train <- training(lgb_split)
lgb_test <- testing(lgb_split)
lgb_folds <- lgb_dataset %>% vfold_cv(v=10, strata = excess_close_rolling_return_1p)

# Definigng model
lgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  # mtry = tune(),
  learn_rate = tune()
) %>%
  set_engine("lightgbm") %>%
  set_mode("regression")

# Defining recipe (currently no steps)
lgb_rec <- recipe(excess_close_rolling_return_1p ~ ., data = lgb_dataset)

# Defining hyperparameter tuning
lgb_params <-
  dials::parameters(
  min_n(),
  tree_depth(),
  learn_rate()
)

lgb_grid <-
  dials::grid_max_entropy(
  lgb_params,
  size = 50
)


lgb_wf <- workflow() %>%
  add_recipe(lgb_rec) %>%
  add_model(lgb_spec)

# Tune
lgb_res <- tune_grid(
  lgb_wf,
  resamples = lgb_folds,
  grid = lgb_grid,
  metrics = yardstick::metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE, verbose = TRUE)
)

saveRDS(lgb_res, file = "lgb_res_saved_2.rds")
```

```{r}
lgb_res <- readRDS(file = "lgb_res_saved_2.rds")
lgb_res %>% show_best(metric = "rmse", n=5)

# Get best hyperparameters after tuning
lgb_best_params <-
  lgb_res %>%
  tune::select_best(metric = "rmse")

# Finalize the workflow 
final_lgb_wf <- finalize_workflow(
  lgb_wf,
  lgb_best_params
)

# Last fit does a final fitting from the finalized workflow, and predicts on the test data as defined by the split. 
final_summary <- final_lgb_wf %>%
  last_fit(split = lgb_split, metrics = metric_set(rmse, rsq, mae)) # Specify your metrics
```

# Check metrics
```{r}
collect_metrics(final_summary)
```


# Analyzing SHAP values for each vairable
```{r}
library(kernelshap)
library(shapviz)
```

```{r}
fitted_workflow_from_last_fit <- extract_workflow(final_summary)

prepped_recipe_object <- extract_recipe(fitted_workflow_from_last_fit)

# xvars <- c('pe', 'evebitda', 'grossmargin', 'ebitdamargin', 'netmargin', 'currentratio')
df_explain <- lgb_train[1:1000, ]  # Use only feature columns

X_pred <- bake(
  prepped_recipe_object,
  new_data = df_explain,
  # Get all predictor columns. Ensure roles are set correctly in your recipe.
  # If your recipe uses has_role("predictor"), this is good.
  # Otherwise, you might need to be more specific or ensure all_predictors() works as expected.
  all_predictors()
) |>
bonsai:::prepare_df_lgbm() # Bonsai specific step for LightGBM

shap_values_lgb <- extract_fit_engine(fitted_workflow_from_last_fit) |>
  shapviz(
    X_pred = X_pred,
    X = df_explain  # This is where df_explain is used
  )

```

```{r}
shap_values_lgb |> 
  sv_importance("bee")

shap_values_lgb |> 
  sv_dependence('pe')
```

